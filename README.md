# scala-dataflow-starter

Despite its widespread use in the foundational design of many modern data processing tools (Apache Spark, Google Dataflow, TensorFlow, Airflow, etc.) the [Dataflow Paradigm](https://en.wikipedia.org/wiki/Dataflow_programming) is hardly ever mentioned in the context of day-to-day Data Engineering. The idea that most data processing problems can be modeled as a series of operations happening in a specific order ([Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)) is by no means a newfound one, however, there's no widespread consensus on how to represent this idea in everyday low-level Data Engineering code (ETL/ELT Pipelines, Monitoring Jobs, Maintenance Jobs, etc.).

This project is my personal interpretation of the Dataflow principles in the context of modular Scala applications, and it aims to be a "starter" app that can be used as as a solid foundation for any everyday Data Engineering project. The terminology I use (DAG, Task) is heavily influenced by [Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html), as it is one of the most widespread large-scale Dataflow-based tools out there. From the semi-infinite number of concrete representations of Dataflow I have chosen one that is built alongside the lines of the [You aren't gonna need it](https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it) principle (e.g. representing Tasks as functions, which access global configuration directly from their definitions) as I have a strong belief that YAGNI is the most commercially viable approach to software development in general.